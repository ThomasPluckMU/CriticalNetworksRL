\subsection{Reinforcement Learning}

Reinforcement Learning (RL) provides a computational framework in which a training agent learns to make sequential decisions through interacting with an enviroment to maximize rewards \cite{mnih2013playingatarideepreinforcement}.
Traditionally, RL problems are modeled as Markov Decision Processes (MDPs) defined by the tuple (\mathcal{S}, \mathcal{A}, P, R), where S is the set of states called the state space, A is the set of actions called the action space, P is the transition probablity function and R is the reward function\cite{Sutton1998}.
The goal is to find a policy \(\pi(a\!\mid\!s)\) that maximizes the expected return, either via the state‑value function
\begin{equation}
V^\pi(s) = \mathbb{E}_\pi\Bigl[\sum_{t=0}^\infty \gamma^t R(s_t,a_t)\mid s_0 = s\Bigr]
\end{equation}
or the action‑value function
\begin{equation}
Q^\pi(s,a) = \mathbb{E}_\pi\Bigl[\sum_{t=0}^\infty \gamma^t R(s_t,a_t)\mid s_0 = s,a_0 = a\Bigr].
\end{equation}

Bellman’s optimality equations characterize the unique fixed point \(Q^*\) (or \(V^*\)) satisfying
\begin{equation}
Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'\!\mid\!s,a)\max_{a'} Q^*(s',a'),
\end{equation}

enabling dynamic‑programming solutions when \(P\) and \(R\) are known \cite{bellman1962applied}.

Q‑learning is a model‑free, off‑policy algorithm that uses this optimality form to iteratively update \(Q(s,a)\) toward \(Q^*\) via

\begin{align}
Q_{t+1}(s_t,a_t)\;&\leftarrow\;Q_t(s_t,a_t)\\
&+ \alpha\Bigl[r_t + \gamma \max_{a'}Q_t(s_{t+1},a') - Q_t(s_t,a_t)\Bigr]
\end{align}

while balancing exploration (e.g.\ \(\varepsilon\)‑greedy) and exploitation \cite{watkins1992q}.  

As a variant of Q-learning, Deep Q-Networks\cite{mnih2013playingatarideepreinforcement} were introduced, employing convolution Neural Networks (CNNs) to approximate \(Q(s,a)\) directly from high-dimensional sensory inputs, emabling end-to-end learning from raw pixels.
DQN was able to demonstrate human-level performance on 49 atari games through experience-replay buffers and periodically updated target networks\cite{mnih2015humanlevel}.

\subsection{Criticality in Neural Systems}

The criticality hypothesis posits that biological neural systems self-organize to operate near critical points between ordered and chaotic dynamics \cite{Beggsetal2003, Beggsetal2012}. Empirical evidence includes observations of "neuronal avalanches" in cortical tissue with size distributions following power laws with exponents of approximately -3/2, matching predictions from critical branching processes \cite{Beggsetal2003}. 

Neural networks near criticality demonstrate optimal computational properties, including maximized dynamic range \cite{Kinouchietal2006, Shewetal2009}, information transmission \cite{Beggsetal2012}, and information storage capacity \cite{Bertschingeretal2004}. Conversely, deviations from criticality correlate with neural pathologies \cite{Meiseletal2011}, suggesting that maintaining criticality is essential for healthy brain function.

These findings motivate our approach: rather than training networks that may accidentally drift away from criticality, we leverage RG flow analysis to design networks that intrinsically maintain critical dynamics throughout operation.

\subsection{Edge of Chaos}

Dynamical systems can be categorized by the exponential rate of divergence of trajectories initially perturbed by a minimal $\delta_0$, leading to asymptotic divergence $|\delta(t)| \approx e^{\lambda t}|\delta(0)|$ - the value of this $\lambda$ known as the Lyapunov exponent characterizes stable $(\lambda < 0)$, chaotic $(\lambda > 0)$ and weakly chaotic or edge of chaos $(\lambda\approx 0)$ regimes\cite{}.

Each of these regimes supports various diffusion statistics on ensembles, with the power law distributions typical of criticality only emerging at the edge of chaos. This makes the edge of chaos a necessary but not necessarily sufficient condition for dynamical systems to exhibit criticality. \cite{}

In ANN systems in particular the weakly chaotic regime can be determined in terms of the $L_2$ norm of the Jacobian of the neural network itself \cite{}. We show in appendix, that in minimizing this specific criterion in terms of ANN parameters, the edge of chaos in ANN systems is actually sufficient for network criticality to arise. 