\subsection{Reinforcement Learning}

Reinforcement Learning (RL) provides a computational framework in which a training agent learns to make sequential decisions through interacting with an enviroment to maximize rewards \cite{mnih2013playingatarideepreinforcement}.
Traditionally, RL problems are modeled as Markov Decision Processes (MDPs) defined by the tuple (\mathcal{S}, \mathcal{A}, P, R), where S is the set of states called the state space, A is the set of actions called the action space, P is the transition probablity function and R is the reward function\cite{Sutton1998}.
The goal is to find a policy \(\pi(a\!\mid\!s)\) that maximizes the expected return, either via the state‑value function
\[
V^\pi(s) = \mathbb{E}_\pi\Bigl[\sum_{t=0}^\infty \gamma^t R(s_t,a_t)\mid s_0 = s\Bigr]
\]
or the action‑value function
\[
Q^\pi(s,a) = \mathbb{E}_\pi\Bigl[\sum_{t=0}^\infty \gamma^t R(s_t,a_t)\mid s_0 = s,a_0 = a\Bigr].
\]
Bellman’s optimality equations characterize the unique fixed point \(Q^*\) (or \(V^*\)) satisfying
\[
Q^*(s,a) = R(s,a) + \gamma \sum_{s'} P(s'\!\mid\!s,a)\max_{a'} Q^*(s',a'),
\]
enabling dynamic‑programming solutions when \(P\) and \(R\) are known \cite{bellman1962applied}.

Q‑learning is a model‑free, off‑policy algorithm that uses this optimality form to iteratively update \(Q(s,a)\) toward \(Q^*\) via
\[
Q_{t+1}(s_t,a_t)\;\leftarrow\;Q_t(s_t,a_t)
+ \alpha\Bigl[r_t + \gamma \max_{a'}Q_t(s_{t+1},a') - Q_t(s_t,a_t)\Bigr]
\]
while balancing exploration (e.g.\ \(\varepsilon\)‑greedy) and exploitation \cite{watkins1992q}.  

As a variant of Q-learning, Deep Q-Networks\cite{mnih2013playingatarideepreinforcement} were introduced, employing convolution Neural Networks (CNNs) to approximate \(Q(s,a)\) directly from high-dimensional sensory inputs, emabling end-to-end learning from raw pixels.
DQN was able to demonstrate human-level performance on 49 atari games through experience-replay buffers and periodically updated target networks\cite{mnih2015humanlevel}.

\subsection{Statistical Mechanics}

Criticality in statistical mechanics refers to phase transitions between states of matter. At criticality, systems exhibit "scale-free" correlations where microscopic and macroscopic scales become indistinguishable, manifesting as power-law distributions and long-range correlations that span all space and time scales of the system.

The Ising model is often used in studying criticality, where atoms in a lattice have spin states $s_i \in \{+1, -1\}$. Each atom $s_i$ couples to others with parameter $J_{ij}$ and experiences local bias from a field $h_i$. The energy of a configuration $\vec{s}$ is:
\begin{equation}E(\vec{s}) = \sum_{i\neq j} J_{ij} s_is_j + \sum_i h_is_i\end{equation}
This defines a Gibbs distribution over the states $\vec{s}$:
\begin{equation}\mathbb{P}(\vec{s})=\dfrac{1}{Z}\exp(-\beta E(\vec{s}))\end{equation}
with partition function $Z$ and inverse pseudotemperature $\beta$.

For neural systems, the mean-field approximation of highly connected Ising models is particularly relevant. In this approximation, the expected value of each spin is:
\begin{equation}
\langle s_i \rangle = \tanh(\beta(\sum_j J_{ij}\langle s_j \rangle + h_i))
\end{equation}
Which can be noted to bear a striking similarity to a hyperbolic tangent artificial neuron:
\begin{equation}
\sigma_i = \tanh(\sum_j w_{ij}x_j + b_i)
\end{equation}
Renormalization Group (RG) analysis allows us to understand scale-invariant behavior by coarse-graining (ie. averaging) the individual spins into blocks of size $b^d$ with block-spin variables given by:
\begin{equation}
S_I = \frac{1}{b^d}\sum_{i \in \text{block } I} s_i
\end{equation} Under this transformation, the effective parameters of the system (coupling strengths $J_{ij}$ and fields $h_i$) change according to RG flow equations $dS_I/db$. A system exhibits critical behavior when its parameters are tuned to values that remain invariant under this transformation  - specifically at the non-trivial fixed point of these flow equations (ie. $dS_I/db=0$) which form a dense subset in parameter space and construct a critical manifold.

\subsection{Criticality in Neural Systems}

The criticality hypothesis posits that biological neural systems self-organize to operate near critical points between ordered and chaotic dynamics \cite{Beggsetal2003, Beggsetal2012}. Empirical evidence includes observations of "neuronal avalanches" in cortical tissue with size distributions following power laws with exponents of approximately -3/2, matching predictions from critical branching processes \cite{Beggsetal2003}. 

Neural networks near criticality demonstrate optimal computational properties, including maximized dynamic range \cite{Kinouchietal2006, Shewetal2009}, information transmission \cite{Beggsetal2012}, and information storage capacity \cite{Bertschingeretal2004}. Conversely, deviations from criticality correlate with neural pathologies \cite{Meiseletal2011}, suggesting that maintaining criticality is essential for healthy brain function.

These findings motivate our approach: rather than training networks that may accidentally drift away from criticality, we leverage RG flow analysis to design networks that intrinsically maintain critical dynamics throughout operation.