\subsection{Edge of Chaos Regularizer}

The proposed method involves the addition of a regularization term to the loss function, which guides the network towards the edge of chaos and in a linear approximation induces criticality - the full derivation can found in the appendix of this paper:

\begin{equation}
R_{(layer)} = \dfrac{2\sigma''(z)\nabla_x^2\sigma(z)}{\sqrt{N}}\left(\dfrac{1}{N}-\dfrac{1}{\|\nabla_x \sigma(z)\|}\right)
\end{equation}

Where $N$ is the number of neurons, $x$ is the input vector, $z$ the pre-activation affine transform of $x$, $\sigma$ an elementwise activation non-linearity and $\nabla$ and $\nabla^2$ the gradient and Laplace operators respectively.

\subsection{Enviroment Setup}

The experiments carried out in this paper uses the Arcade Learning Enviroment (ALE), which provides a standardized interface to 26 of atari 2600 games as challenge problems for general agents\cite{Bellemare_2013}.
Each game is instantiated via the OpenAi Gym API with deterministic frame-skipping wrappers to ensure consistent dynamics and reproducibility.
To handle the varying control schemes across different games, the maximum discrete actionspace is computed among the verified enviroments.
Observations of raw frames (210×160×3) are converted to grayscale, resized to 84x84 pixels and max-pooled over two consecutive frames to mitigate flickering artifacts before stacking four  frames into an 84x84x4 tensor to encode short-term motion\cite{terry2020arcade}
Rewards are clipped to [-1, +1] to bound temporal-difference targets across games with widely varying reward scales, and actions are taken from the minimal discrete action set, through Gymnasium's API.

\subsection{Experiments}