\section{Derivation of the Criticality Regularizer}

In this appendix, we provide a rigorous derivation of our proposed regularization term that promotes criticality in neural networks. Our approach drives networks to the edge of chaos by explicit Jacobian constraints, incidentally the derived regularizer also pushes the network towards scale-free dynamics - providing a  tentative theoretical justification that the edge of chaos may be sufficient for criticality in ANNs.

\subsection{Regularizing to the Edge of Chaos}

Let us define a standard feedforward network $a=\sigma(z)$ where preactivation $z=Wx+b$ is defined with weight matrix $W$, bias $b$, input $x$ and the put through non-linearity $\sigma$ to create activation $a$. A known fact about rank-$N$ operators $J$ is that their Lyapunov exponent collapse when $\|J\|_F^2 = N$ at the so-called "edge of chaos".

We derive our regularizer by letting $J$ be the Jacobian of the feedforward layer $a=\sigma(z)$ and finding explicit derivatives in terms of weights $W$ and biases $b$ to minimize the quantity $J$ - to simplify derivation we will focus entirely on individual entries of $b$ the $b_i$ and assure the reader that much same terms will arise when computing the derivative of $W_{ij}$

Let us begin by computing the derivative of $a_i$ with respect to $x_j$ for the individual terms of the Jacobian $J_{ij}$:
\begin{equation}
J_{ij}=\dfrac{\partial}{\partial x_j} \sigma(z_i) = W_{ij}\sigma'(z_i)
\end{equation}

We can now compute the Frobenius norm of $J$ and begin computing it's derivative w.r.t. $b_i$:

\begin{align}
\dfrac{\partial}{\partial b_i}\|J\|_F &= \dfrac{\partial}{\partial b_i} \sqrt{\sum_{i,j}W_{ij}^2\sigma'(z_i)^2}\\
&= \dfrac{\sum_{j} W_{ij}^2 \sigma'(z_i) \sigma''(z_i)}{\|J\|_F}
\end{align}

We note at this juncture that $\frac{\partial^2}{\partial x_j^2} \sigma(z_i) = W_{ij}^2 \sigma''(z_i)$ so we may write:

\begin{equation}
\dfrac{\partial}{\partial b_i}\|J\|_F = \dfrac{\sigma'(z_i) \nabla^2\sigma(z_i)}{\|J\|_F}
\end{equation}

Where $\nabla^2$ is the Laplace operator. We would now like to encode the edge of chaos criterion $\|J\|_F^2=N$ into an explicit quantity that we can minimize using the parameters of our network.

\begin{align}
\dfrac{\partial}{\partial b_i} \left(1-\dfrac{\|J\|_F}{\sqrt{N}}\right)^2 &=  \dfrac{\partial}{\partial b_i}\left(1-\dfrac{2\|J\|_F}{\sqrt{N}}+\dfrac{\|J\|_F^2}{N}\right)\\
&= 2\dfrac{\partial}{\partial b_i}\|J\|_F\cdot\dfrac{\|J\|_F}{N} - \dfrac{\partial}{\partial b_i} \dfrac{2\|J\|_F}{\sqrt{N}}\\
&= \dfrac{2 \sigma'(z_i)\nabla^2 \sigma(z_i)}{\sqrt{N}}\left(\dfrac{1}{N}-\dfrac{1}{\|J\|_F}\right)
\end{align}

\subsection{Properties of the Regularizer}

At this juncture we can begin to discuss the properties of the proposed regularizer - while it clearly regularises the network toward the edge of chaos, it is not immediately clear from the form derived that minimizing this quantity leads to criticality and scale-free phenomena.

The simplest observation comes from understanding the nature of the Laplacian and Jacobian when applied to re-scaled linear maps. Given some linear map $L:\mathbb{R}^n\to \mathbb{R}^n$, the effect of rescaling this map as $L'(x) = L(x/\alpha)$ has a quadratic action on the Laplacian such that $\nabla^2 L'(x) = \nabla^2 L(x/\alpha) = (1/\alpha^2)\nabla^2 L(x/\alpha)$, and similarly a linear action on the Jacobian where $J_{L'}(x) = J_L(x/\alpha) \cdot (1/\alpha)I = (1/\alpha)J_L(x/\alpha)$. This demonstrates that the Laplacian scales by a factor of $1/\alpha^2$ while the Jacobian scales by a factor of $1/\alpha$ under coordinate rescaling.

So looking at the derived form above, we can see it as the difference between a quadratic and linear scaling operator (at least in the linear sense) - if we then attempt to minimize this quantity, it follows that we are trying to find a regime in which linear and quadratic scaling are equivalent, ie. the system is scale-free.

The extent that this analysis can be extended to the full quasi-linear case seen in neural networks would require a complete renormalization group analysis to formalize this connection rigorously, as the non-linearities in neural networks introduce complexities beyond the simplified linear map case presented here.